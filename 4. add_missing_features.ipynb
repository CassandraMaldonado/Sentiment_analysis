{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to improve each articles analysis with more features that go beyond text. I created dictionaries to detect industries, job roles, technologies and organizations to perform my custom sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache directory: /Users/casey/Documents/GitHub/AI_impact_employment/cache\n"
     ]
    }
   ],
   "source": [
    "# Cache directory.\n",
    "cache_dir = \"cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "print(f\"Using cache directory: {os.path.abspath(cache_dir)}\")\n",
    "\n",
    "def get_cache_path(filename):\n",
    "    return os.path.join(cache_dir, filename)\n",
    "\n",
    "def save_to_cache(obj, filename):\n",
    "    with open(get_cache_path(filename), 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "    print(f\"Saved {filename} to cache\")\n",
    "\n",
    "def load_from_cache(filename):\n",
    "    cache_path = get_cache_path(filename)\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary Functions.\n",
    "\n",
    "def create_sentiment_dictionaries():\n",
    "    print(\"Creating sentiment dictionaries.\")\n",
    "    \n",
    "    # Positive terms related to AI in the workplace context.\n",
    "    positive_terms = {\n",
    "        # Opportunity and growth.\n",
    "        'opportunity': 1.0, 'enhance': 0.8, 'improve': 0.8, 'augment': 0.7,\n",
    "        'growth': 0.7, 'advancement': 0.8, 'upskill': 0.9, 'progress': 0.7,\n",
    "        'potential': 0.5, 'revolutionize': 0.8, 'transform': 0.7,\n",
    "\n",
    "        # Productivity and efficiency.\n",
    "        'efficiency': 0.8, 'productivity': 0.8, 'streamline': 0.7,\n",
    "        'optimize': 0.7, 'accelerate': 0.6, 'automate': 0.6,\n",
    "\n",
    "        # Collaboration and assistance.\n",
    "        'assist': 0.6, 'empower': 0.9, 'collaborate': 0.7, 'partnership': 0.6,\n",
    "        'complement': 0.7, 'teamwork': 0.7, 'support': 0.6, 'aid': 0.6,\n",
    "\n",
    "        # Solution and benefit.\n",
    "        'solution': 0.6, 'benefit': 0.8, 'advantage': 0.7, 'value': 0.6,\n",
    "        'solve': 0.7, 'facilitate': 0.6, 'enable': 0.7,\n",
    "\n",
    "        # Innovation and creation.\n",
    "        'innovation': 0.9, 'create': 0.6, 'invent': 0.7, 'develop': 0.6,\n",
    "        'pioneer': 0.8, 'breakthrough': 0.9, 'novel': 0.7\n",
    "    }\n",
    "\n",
    "    # Negative terms related to AI in the workplace context.\n",
    "    negative_terms = {\n",
    "        # Job Loss and replacement.\n",
    "        'replace': -0.8, 'eliminate': -0.9, 'displace': -0.8, 'substitute': -0.7,\n",
    "        'job loss': -0.9, 'unemployment': -0.9, 'layoff': -0.9, 'redundant': -0.8,\n",
    "        'downsizing': -0.8, 'obsolete': -0.8, 'outdated': -0.7,\n",
    "\n",
    "        # Risk and threat.\n",
    "        'threaten': -0.7, 'risk': -0.6, 'danger': -0.7, 'concern': -0.5,\n",
    "        'worry': -0.6, 'fear': -0.7, 'threat': -0.8, 'harmful': -0.8,\n",
    "\n",
    "        # Problems and challenges.\n",
    "        'controversy': -0.6, 'problem': -0.6, 'challenge': -0.4, 'difficulty': -0.5,\n",
    "        'obstacle': -0.5, 'hurdle': -0.4, 'barrier': -0.5,\n",
    "\n",
    "        # Social issues.\n",
    "        'inequality': -0.7, 'bias': -0.7, 'discrimination': -0.8, 'unfair': -0.7,\n",
    "        'disparity': -0.7, 'divide': -0.6, 'exclusion': -0.7,\n",
    "\n",
    "        # Control and privacy.\n",
    "        'surveillance': -0.8, 'monitor': -0.6, 'control': -0.6, 'invasion': -0.7,\n",
    "        'privacy': -0.7, 'intrusive': -0.7, 'oversight': -0.5\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'positive_terms': positive_terms,\n",
    "        'negative_terms': negative_terms\n",
    "    }\n",
    "\n",
    "# Industry dictionaries based on my knowledge and common industry terms.\n",
    "def create_industry_dictionaries():\n",
    "    print(\"Creating industry dictionaries.\")\n",
    "    \n",
    "    industry_terms = {\n",
    "        'healthcare': [\n",
    "            'doctor', 'physician', 'nurse', 'hospital', 'clinic', 'patient', 'care',\n",
    "            'medical', 'healthcare', 'health care', 'medicine', 'pharma', 'clinical'\n",
    "        ],\n",
    "\n",
    "        'finance': [\n",
    "            'bank', 'banking', 'investment', 'investor', 'loan', 'credit', \n",
    "            'financial', 'finance', 'trading', 'insurance', 'fintech'\n",
    "        ],\n",
    "\n",
    "        'manufacturing': [\n",
    "            'factory', 'manufacturing', 'production', 'assembly', 'supply chain',\n",
    "            'industrial', 'automotive', 'machinery', 'robotics', 'automation'\n",
    "        ],\n",
    "\n",
    "        'retail': [\n",
    "            'store', 'shop', 'retail', 'e-commerce', 'customer', 'consumer',\n",
    "            'inventory', 'merchandising', 'commerce', 'shopping'\n",
    "        ],\n",
    "\n",
    "        'education': [\n",
    "            'school', 'university', 'college', 'student', 'teacher', 'professor',\n",
    "            'education', 'learning', 'teaching', 'training', 'academic'\n",
    "        ],\n",
    "\n",
    "        'technology': [\n",
    "            'software', 'hardware', 'tech', 'technology', 'computer', 'digital',\n",
    "            'it', 'internet', 'web', 'app', 'computing', 'cloud'\n",
    "        ],\n",
    "\n",
    "        'media': [\n",
    "            'media', 'news', 'entertainment', 'publishing', 'content', \n",
    "            'social media', 'journalist', 'writing', 'advertising'\n",
    "        ],\n",
    "\n",
    "        'legal': [\n",
    "            'legal', 'lawyer', 'attorney', 'law firm', 'regulatory', 'compliance',\n",
    "            'court', 'litigation', 'judge', 'justice', 'contract'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    industry_term_weights = {\n",
    "        'healthcare': {'hospital': 5, 'doctor': 4, 'patient': 3, 'medical': 2, 'healthcare': 5},\n",
    "        'finance': {'bank': 5, 'investment': 4, 'financial': 3, 'loan': 2, 'finance': 5},\n",
    "        'manufacturing': {'factory': 5, 'manufacturing': 5, 'production': 4, 'assembly': 3},\n",
    "        'retail': {'store': 4, 'retail': 5, 'e-commerce': 5, 'consumer': 3},\n",
    "        'education': {'school': 5, 'university': 5, 'student': 4, 'education': 5},\n",
    "        'technology': {'software': 4, 'tech': 5, 'technology': 5, 'digital': 3},\n",
    "        'media': {'media': 5, 'news': 4, 'content': 3, 'publishing': 4},\n",
    "        'legal': {'lawyer': 5, 'legal': 5, 'law': 4, 'attorney': 5}\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'industry_terms': industry_terms,\n",
    "        'industry_term_weights': industry_term_weights\n",
    "    }\n",
    "\n",
    "# Job and technology dictionaries based on common job roles.\n",
    "def create_job_dictionaries():\n",
    "    print(\"Creating job dictionaries.\")\n",
    "    \n",
    "    job_terms = {\n",
    "        'management': [\n",
    "            'ceo', 'chief executive', 'cfo', 'cio', 'cto', 'coo', 'executive',\n",
    "            'manager', 'supervisor', 'director', 'leadership', 'administration'\n",
    "        ],\n",
    "\n",
    "        'engineering': [\n",
    "            'engineer', 'developer', 'programmer', 'coder', 'data scientist',\n",
    "            'machine learning engineer', 'ai engineer', 'software engineer',\n",
    "            'technical', 'architect', 'DevOps'\n",
    "        ],\n",
    "\n",
    "        'creative': [\n",
    "            'designer', 'writer', 'artist', 'content creator', 'creative',\n",
    "            'marketer', 'marketing', 'advertiser', 'author', 'editor'\n",
    "        ],\n",
    "\n",
    "        'education': [\n",
    "            'teacher', 'professor', 'instructor', 'educator', 'faculty',\n",
    "            'academic', 'trainer', 'teaching', 'tutor', 'lecturer'\n",
    "        ],\n",
    "\n",
    "        'healthcare': [\n",
    "            'doctor', 'nurse', 'physician', 'surgeon', 'medical professional',\n",
    "            'pharmacist', 'therapist', 'healthcare worker', 'clinician'\n",
    "        ],\n",
    "\n",
    "        'finance': [\n",
    "            'banker', 'accountant', 'financial analyst', 'trader', 'investor',\n",
    "            'broker', 'financial advisor', 'auditor', 'actuary'\n",
    "        ],\n",
    "\n",
    "        'service': [\n",
    "            'customer service', 'retail worker', 'sales associate', 'cashier',\n",
    "            'receptionist', 'assistant', 'representative', 'clerk'\n",
    "        ],\n",
    "\n",
    "        'manufacturing': [\n",
    "            'factory worker', 'machine operator', 'assembler', 'production worker',\n",
    "            'technician', 'mechanic', 'quality control', 'maintenance'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'job_terms': job_terms\n",
    "    }\n",
    "\n",
    "# Technology dictionaries based on common AI and technology words.\n",
    "def create_technology_dictionaries():\n",
    "    print(\"Creating technology dictionaries.\")\n",
    "    \n",
    "    technology_terms = {\n",
    "        'machine_learning': [\n",
    "            'machine learning', 'ml', 'artificial intelligence', 'ai', 'algorithm',\n",
    "            'deep learning', 'neural network', 'data science'\n",
    "        ],\n",
    "\n",
    "        'nlp': [\n",
    "            'natural language processing', 'nlp', 'language model', 'llm',\n",
    "            'large language model', 'chatbot', 'gpt', 'bert'\n",
    "        ],\n",
    "\n",
    "        'computer_vision': [\n",
    "            'computer vision', 'image recognition', 'object detection',\n",
    "            'facial recognition', 'image processing'\n",
    "        ],\n",
    "\n",
    "        'robotics': [\n",
    "            'robot', 'robotics', 'automation', 'autonomous', 'self-driving',\n",
    "            'robotic process automation', 'rpa'\n",
    "        ],\n",
    "\n",
    "        'ai_infrastructure': [\n",
    "            'gpu', 'cloud computing', 'edge computing', 'federated learning',\n",
    "            'ai chip', 'compute', 'transformer'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    ai_models = [\n",
    "        'gpt', 'chatgpt', 'gpt-4', 'gpt-3', 'dall-e', 'bard', 'palm',\n",
    "        'llama', 'claude', 'stable diffusion', 'midjourney', 'gemini'\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'technology_terms': technology_terms,\n",
    "        'ai_models': ai_models\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction Functions\n",
    "# Detecting industries.\n",
    "def detect_industries(text, industry_terms, industry_term_weights=None):\n",
    "    if not text or pd.isna(text):\n",
    "        return []\n",
    "        \n",
    "    text_lower = text.lower()\n",
    "    category_scores = defaultdict(float)\n",
    "    \n",
    "    for category, terms in industry_terms.items():\n",
    "        for term in terms:\n",
    "            count = text_lower.count(term)\n",
    "            if count > 0:\n",
    "                weight = 1.0\n",
    "                if industry_term_weights and category in industry_term_weights and term in industry_term_weights[category]:\n",
    "                    weight = industry_term_weights[category][term]\n",
    "                \n",
    "                length_weight = min(1.0, 0.5 + len(term) / 20.0)\n",
    "                score = count * weight * length_weight\n",
    "                category_scores[category] += score\n",
    "    \n",
    "    sorted_categories = sorted(category_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [category for category, _ in sorted_categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting jobs.\n",
    "def detect_jobs(text, job_terms):\n",
    "    if not text or pd.isna(text):\n",
    "        return []\n",
    "        \n",
    "    text_lower = text.lower()\n",
    "    category_scores = defaultdict(float)\n",
    "    \n",
    "    for category, terms in job_terms.items():\n",
    "        for term in terms:\n",
    "            count = text_lower.count(term)\n",
    "            if count > 0:\n",
    "                length_weight = min(1.0, 0.5 + len(term) / 20.0)\n",
    "                score = count * length_weight\n",
    "                category_scores[category] += score\n",
    "    \n",
    "    sorted_categories = sorted(category_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [category for category, _ in sorted_categories]\n",
    "\n",
    "# Detecting technologies.\n",
    "def identify_technologies(text, technology_terms, ai_models):\n",
    "    if not text or pd.isna(text):\n",
    "        return {}\n",
    "        \n",
    "    text_lower = text.lower()\n",
    "    found_techs = {}\n",
    "    \n",
    "    for tech_category, keywords in technology_terms.items():\n",
    "        matched_keywords = [k for k in keywords if k in text_lower]\n",
    "        if matched_keywords:\n",
    "            matched_keywords.sort(key=len, reverse=True)\n",
    "            found_techs[tech_category] = matched_keywords\n",
    "    \n",
    "    found_models = [model for model in ai_models if model.lower() in text_lower]\n",
    "    if found_models:\n",
    "        found_techs['specific_models'] = found_models\n",
    "    \n",
    "    return found_techs\n",
    "\n",
    "# Detecting organizations.\n",
    "def extract_organizations(text):\n",
    "    if not text or pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    known_orgs = [\n",
    "        'OpenAI', 'Google', 'Microsoft', 'Apple', 'Amazon', 'Meta', 'Facebook',\n",
    "        'IBM', 'Anthropic', 'NVIDIA', 'Intel', 'AMD', 'Tesla', 'DeepMind'\n",
    "    ]\n",
    "    \n",
    "    found_orgs = []\n",
    "    for org in known_orgs:\n",
    "        if org.lower() in text.lower():\n",
    "            found_orgs.append(org)\n",
    "    \n",
    "    return found_orgs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "# Managing negation detection for speed.\n",
    "def fast_detect_negations(text, target_terms):\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    negated_terms = []\n",
    "    \n",
    "    # Negation words.\n",
    "    negation_words = ['not', 'never', 'no', \"don't\", \"doesn't\", \"didn't\", \"won't\", \"can't\"]\n",
    "    \n",
    "    for neg_word in negation_words:\n",
    "        if neg_word in text_lower:\n",
    "            # Looking for sentiment terms within 30 characters after negation.\n",
    "            for term in target_terms:\n",
    "                pattern = f\"{neg_word}.{{0,30}}{re.escape(term)}\"\n",
    "                if re.search(pattern, text_lower):\n",
    "                    negated_terms.append(term)\n",
    "    \n",
    "    return negated_terms\n",
    "\n",
    "# Proximity analysis.\n",
    "def fast_proximity_analysis(text, positive_terms, negative_terms):\n",
    "    if not text:\n",
    "        return 0\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    sentences = text.split('.')\n",
    "    \n",
    "    ai_terms = ['ai', 'artificial intelligence', 'machine learning', 'automation']\n",
    "    impact_terms = ['job', 'work', 'employee', 'career', 'industry', 'employment']\n",
    "    \n",
    "    proximity_scores = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_lower = sentence.lower()\n",
    "        has_ai = any(term in sentence_lower for term in ai_terms)\n",
    "        has_impact = any(term in sentence_lower for term in impact_terms)\n",
    "        \n",
    "        if has_ai and has_impact:\n",
    "            # Sentiment scoring.\n",
    "            sentence_score = 0\n",
    "            for term, value in positive_terms.items():\n",
    "                if term in sentence_lower:\n",
    "                    sentence_score += value\n",
    "            for term, value in negative_terms.items():\n",
    "                if term in sentence_lower:\n",
    "                    sentence_score += value\n",
    "            \n",
    "            proximity_scores.append(sentence_score)\n",
    "    \n",
    "    return np.mean(proximity_scores) if proximity_scores else 0\n",
    "\n",
    "# Custom sentiment analysis.\n",
    "def fast_enhanced_sentiment_analysis(text, positive_terms, negative_terms, industry=None):\n",
    "    if not text or pd.isna(text):\n",
    "        return {\n",
    "            'overall': 0,\n",
    "            'base_textblob': 0,\n",
    "            'lexicon_normalized': 0,\n",
    "            'proximity_enhanced': 0,\n",
    "            'recency_weighted': 0,\n",
    "            'negation_adjusted': 0,\n",
    "            'word_count': 0,\n",
    "            'negated_terms_count': 0\n",
    "        }\n",
    "    \n",
    "    # Base sentiment from TextBlob.\n",
    "    base_sentiment = TextBlob(text).sentiment.polarity\n",
    "    \n",
    "    # Normalized lexicon analysis.\n",
    "    text_lower = text.lower()\n",
    "    word_count = len(text_lower.split())\n",
    "    \n",
    "    positive_score = 0\n",
    "    positive_matches = 0\n",
    "    negative_score = 0\n",
    "    negative_matches = 0\n",
    "    \n",
    "    all_sentiment_terms = list(positive_terms.keys()) + list(negative_terms.keys())\n",
    "    \n",
    "    # Negation detection.\n",
    "    negated_terms = fast_detect_negations(text, all_sentiment_terms)\n",
    "    \n",
    "    # Scored terms.\n",
    "    for term, value in positive_terms.items():\n",
    "        count = text_lower.count(term)\n",
    "        if count > 0:\n",
    "            if term in negated_terms:\n",
    "                value *= -0.5\n",
    "            positive_matches += count\n",
    "            positive_score += value * count\n",
    "    \n",
    "    for term, value in negative_terms.items():\n",
    "        count = text_lower.count(term)\n",
    "        if count > 0:\n",
    "            if term in negated_terms:\n",
    "                value *= -0.5\n",
    "            negative_matches += count\n",
    "            negative_score += value * count\n",
    "    \n",
    "    # Normalized by text length.\n",
    "    total_matches = positive_matches + negative_matches\n",
    "    if total_matches > 0 and word_count > 0:\n",
    "        normalization_factor = np.log(word_count + 1)\n",
    "        lexicon_normalized = (positive_score + negative_score) / (total_matches * normalization_factor)\n",
    "    else:\n",
    "        lexicon_normalized = 0\n",
    "    \n",
    "    # Proximity analysis.\n",
    "    proximity_enhanced = fast_proximity_analysis(text, positive_terms, negative_terms)\n",
    "    \n",
    "    # Recency weight for AI terms appearing later. If the term appears in the last 60% of the text.\n",
    "    recency_weight = 1.0\n",
    "    ai_terms = ['ai', 'artificial intelligence', 'machine learning']\n",
    "    for term in ai_terms:\n",
    "        last_pos = text_lower.rfind(term)\n",
    "        if last_pos > len(text) * 0.6:  \n",
    "            recency_weight = 1.3\n",
    "            break\n",
    "    \n",
    "    recency_weighted = lexicon_normalized * recency_weight\n",
    "    \n",
    "    # Negation adjustment.\n",
    "    negation_penalty = len(negated_terms) * 0.1\n",
    "    negation_adjusted = lexicon_normalized - negation_penalty\n",
    "    \n",
    "    # Combined score.\n",
    "    weights = {\n",
    "        'base_textblob': 0.35,       \n",
    "        'lexicon_normalized': 0.30,   \n",
    "        'proximity_enhanced': 0.25,\n",
    "        'recency_weighted': 0.05,\n",
    "        'negation_adjusted': 0.05\n",
    "    }\n",
    "    \n",
    "    overall_sentiment = (\n",
    "        weights['base_textblob'] * base_sentiment +\n",
    "        weights['lexicon_normalized'] * lexicon_normalized +\n",
    "        weights['proximity_enhanced'] * proximity_enhanced +\n",
    "        weights['recency_weighted'] * (recency_weighted - lexicon_normalized) +\n",
    "        weights['negation_adjusted'] * (negation_adjusted - lexicon_normalized)\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'overall': overall_sentiment,\n",
    "        'base_textblob': base_sentiment,\n",
    "        'lexicon_normalized': lexicon_normalized,\n",
    "        'proximity_enhanced': proximity_enhanced,\n",
    "        'recency_weighted': recency_weighted,\n",
    "        'negation_adjusted': negation_adjusted,\n",
    "        'word_count': word_count,\n",
    "        'negated_terms_count': len(negated_terms)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Functions\n",
    "# Dictionaries for the enhanced features.\n",
    "def create_dictionaries():\n",
    "    dictionaries = {}\n",
    "    \n",
    "    dictionaries['sentiment'] = create_sentiment_dictionaries()\n",
    "    dictionaries['industry'] = create_industry_dictionaries()\n",
    "    dictionaries['job'] = create_job_dictionaries()\n",
    "    dictionaries['technology'] = create_technology_dictionaries()\n",
    "    \n",
    "    return dictionaries\n",
    "\n",
    "# Added the enhanced features to the dataset.\n",
    "def add_fast_enhanced_features_to_dataset(df, dictionaries):\n",
    "    \n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    sentiment_dict = dictionaries['sentiment']\n",
    "    industry_dict = dictionaries['industry']\n",
    "    job_dict = dictionaries['job']\n",
    "    technology_dict = dictionaries['technology']\n",
    "    \n",
    "    # Feature detection.\n",
    "    print(\"Detecting industries and jobs.\")\n",
    "    tqdm.pandas(desc=\"Industries\")\n",
    "    df_enhanced['detected_industries'] = df_enhanced['cleaned_text'].progress_apply(\n",
    "        lambda x: detect_industries(\n",
    "            x, \n",
    "            industry_dict['industry_terms'], \n",
    "            industry_dict['industry_term_weights']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    tqdm.pandas(desc=\"Jobs\")\n",
    "    df_enhanced['detected_jobs'] = df_enhanced['cleaned_text'].progress_apply(\n",
    "        lambda x: detect_jobs(x, job_dict['job_terms'])\n",
    "    )\n",
    "    \n",
    "    print(\"Identifying AI technologies.\")\n",
    "    tqdm.pandas(desc=\"Technologies\")\n",
    "    df_enhanced['ai_technologies'] = df_enhanced['cleaned_text'].progress_apply(\n",
    "        lambda x: identify_technologies(\n",
    "            x, \n",
    "            technology_dict['technology_terms'],\n",
    "            technology_dict['ai_models']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(\"Extracting organizations.\")\n",
    "    tqdm.pandas(desc=\"Organizations\")\n",
    "    df_enhanced['top_organizations'] = df_enhanced['cleaned_text'].progress_apply(extract_organizations)\n",
    "    \n",
    "    print(\"Analyzing sentiment with fast enhanced model.\")\n",
    "    \n",
    "    tqdm.pandas(desc=\"Sentiment Analysis\")\n",
    "    df_enhanced['enhanced_sentiment_scores'] = df_enhanced.progress_apply(\n",
    "        lambda x: fast_enhanced_sentiment_analysis(\n",
    "            x['cleaned_text'],\n",
    "            sentiment_dict['positive_terms'],\n",
    "            sentiment_dict['negative_terms'],\n",
    "            x['detected_industries'][0] if len(x['detected_industries']) > 0 else None\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Extracted sentiment scores.\n",
    "    df_enhanced['sentiment_overall_enhanced'] = df_enhanced['enhanced_sentiment_scores'].apply(lambda x: x['overall'])\n",
    "    df_enhanced['sentiment_base_textblob'] = df_enhanced['enhanced_sentiment_scores'].apply(lambda x: x['base_textblob'])\n",
    "    df_enhanced['sentiment_lexicon_normalized'] = df_enhanced['enhanced_sentiment_scores'].apply(lambda x: x['lexicon_normalized'])\n",
    "    df_enhanced['sentiment_proximity_enhanced'] = df_enhanced['enhanced_sentiment_scores'].apply(lambda x: x['proximity_enhanced'])\n",
    "    df_enhanced['sentiment_recency_weighted'] = df_enhanced['enhanced_sentiment_scores'].apply(lambda x: x['recency_weighted'])\n",
    "    df_enhanced['sentiment_negation_adjusted'] = df_enhanced['enhanced_sentiment_scores'].apply(lambda x: x['negation_adjusted'])\n",
    "    \n",
    "    # Additional features.\n",
    "    df_enhanced['article_word_count'] = df_enhanced['enhanced_sentiment_scores'].apply(lambda x: x['word_count'])\n",
    "    df_enhanced['negated_terms_count'] = df_enhanced['enhanced_sentiment_scores'].apply(lambda x: x['negated_terms_count'])\n",
    "    \n",
    "    # Primary categories for industries and jobs.\n",
    "    df_enhanced['primary_industry'] = df_enhanced['detected_industries'].apply(\n",
    "        lambda x: x[0] if len(x) > 0 else None\n",
    "    )\n",
    "    \n",
    "    df_enhanced['primary_job'] = df_enhanced['detected_jobs'].apply(\n",
    "        lambda x: x[0] if len(x) > 0 else None\n",
    "    )\n",
    "    \n",
    "    return df_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fast_enhanced_pipeline():\n",
    "    \n",
    "    # Loaded data.\n",
    "    df = load_from_cache('data_with_topics.pkl')\n",
    "    if df is None:\n",
    "        print(\"Error: Could not load data.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Loaded data with {len(df)} articles\")\n",
    "    \n",
    "    # Created the dictionaries.\n",
    "    dictionaries = create_dictionaries()\n",
    "    \n",
    "    # Added features.\n",
    "    df_enhanced = add_fast_enhanced_features_to_dataset(df, dictionaries)\n",
    "    \n",
    "    # Saved dataset.\n",
    "    save_to_cache(df_enhanced, 'fast_enhanced_data_with_features.pkl')\n",
    "    \n",
    "    print(\"Fast enhanced pipeline completed.\")\n",
    "    print(\"Enhanced data saved to 'fast_enhanced_data_with_features.pkl'\")\n",
    "    \n",
    "    return df_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting FAST enhanced pipeline...\n",
      "Loaded data with 184391 articles\n",
      "Creating sentiment dictionaries...\n",
      "Creating industry dictionaries...\n",
      "Creating job dictionaries...\n",
      "Creating technology dictionaries...\n",
      "Adding fast enhanced features to dataset...\n",
      "Detecting industries and jobs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a88c4b25d04d3d8237d2ed806f6a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Industries:   0%|          | 0/184391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749cc06da85c402486c1d64300efc8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Jobs:   0%|          | 0/184391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifying AI technologies...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739e444d01384884b839c7666fab8ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Technologies:   0%|          | 0/184391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting organizations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66141c435da41deac6eb6f063a62380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Organizations:   0%|          | 0/184391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing sentiment with fast enhanced model...\n",
      "Note: Using TextBlob + enhanced features (NO FinBERT for speed)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777278e9f1ec44f7823d6645bfabdb08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sentiment Analysis:   0%|          | 0/184391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fast_enhanced_data_with_features.pkl to cache\n",
      "Fast enhanced pipeline complete!\n",
      "Enhanced data saved to 'fast_enhanced_data_with_features.pkl'\n",
      "Pipeline completed successfully!\n",
      "Enhanced dataset shape: (184391, 29)\n"
     ]
    }
   ],
   "source": [
    "# Runing the pipeline.\n",
    "if __name__ == \"__main__\":\n",
    "    df_enhanced = run_fast_enhanced_pipeline()\n",
    "    \n",
    "    if df_enhanced is not None:\n",
    "        print(\"Pipeline completed successfully.\")\n",
    "        print(f\"Enhanced dataset shape: {df_enhanced.shape}\")\n",
    "    else:\n",
    "        print(\"Pipeline failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Industry Categories Summary:\n",
      "technology: 184354 articles\n",
      "media: 180245 articles\n",
      "education: 146171 articles\n",
      "healthcare: 128916 articles\n",
      "retail: 125436 articles\n",
      "finance: 122744 articles\n",
      "legal: 118712 articles\n",
      "manufacturing: 81758 articles\n",
      "Total unique industries detected: 8\n"
     ]
    }
   ],
   "source": [
    "# Industry categories and times.\n",
    "# Industry categories and their counts from the dataset.\n",
    "def get_industry_categories_and_times(df):\n",
    "    if 'detected_industries' not in df.columns:\n",
    "        print(\"Error: detected industries column not found in dataset.\")\n",
    "        return None\n",
    "    \n",
    "    industry_counts = Counter()\n",
    "    \n",
    "    for industries in df['detected_industries']:\n",
    "        for industry in industries:\n",
    "            industry_counts[industry] += 1\n",
    "    \n",
    "    # Sorted by count.\n",
    "    sorted_industries = sorted(industry_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_industries\n",
    "\n",
    "# Summary of industry categories and their counts.\n",
    "def print_industry_summary(df):\n",
    "    industry_summary = get_industry_categories_and_times(df)\n",
    "    \n",
    "    if industry_summary is None:\n",
    "        print(\"No industry data available.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Industry Categories Summary:\")\n",
    "    for industry, count in industry_summary:\n",
    "        print(f\"{industry}: {count} articles\")\n",
    "    \n",
    "    print(f\"Total unique industries: {len(industry_summary)}\")\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the dataset.\n",
    "    df_enhanced = load_from_cache('fast_enhanced_data_with_features.pkl')\n",
    "    \n",
    "    if df_enhanced is not None:\n",
    "        print_industry_summary(df_enhanced)\n",
    "    else:\n",
    "        print(\"Error: Could not load the dataset.\")\n",
    "        print(\"Run the fast enhanced pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Types Summary:\n",
      "management: 165572 articles\n",
      "creative: 124566 articles\n",
      "engineering: 82332 articles\n",
      "education: 47065 articles\n",
      "finance: 46949 articles\n",
      "service: 39692 articles\n",
      "healthcare: 19718 articles\n",
      "manufacturing: 11444 articles\n",
      "Total unique job types detected: 8\n"
     ]
    }
   ],
   "source": [
    "# Job types and times.\n",
    "# Job types and their counts.\n",
    "def get_job_types_and_times(df):\n",
    "    if 'detected_jobs' not in df.columns:\n",
    "        print(\"Error: detected jobs column not found in the dataset.\")\n",
    "        return None\n",
    "    \n",
    "    job_counts = Counter()\n",
    "    \n",
    "    for jobs in df['detected_jobs']:\n",
    "        for job in jobs:\n",
    "            job_counts[job] += 1\n",
    "    \n",
    "    # Sorted by count.\n",
    "    sorted_jobs = sorted(job_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_jobs\n",
    "\n",
    "# Summary of job types and their counts.\n",
    "def print_job_summary(df):\n",
    "    job_summary = get_job_types_and_times(df)\n",
    "    \n",
    "    if job_summary is None:\n",
    "        print(\"No job data available.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Job Types Summary:\")\n",
    "    for job, count in job_summary:\n",
    "        print(f\"{job}: {count} articles\")\n",
    "    \n",
    "    print(f\"Total unique job types: {len(job_summary)}\")\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the dataset.\n",
    "    df_enhanced = load_from_cache('fast_enhanced_data_with_features.pkl')\n",
    "    \n",
    "    if df_enhanced is not None:\n",
    "        print_job_summary(df_enhanced)\n",
    "    else:\n",
    "        print(\"Error: Could not load enhanced dataset.\")\n",
    "        print(\"Run the fast enhanced pipeline.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
